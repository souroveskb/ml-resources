{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/pytorch/vision/zipball/v0.12.0\" to /home/sourove/.cache/torch/hub/v0.12.0.zip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alexnet\n",
      "convnext_base\n",
      "convnext_large\n",
      "convnext_small\n",
      "convnext_tiny\n",
      "deeplabv3_mobilenet_v3_large\n",
      "deeplabv3_resnet101\n",
      "deeplabv3_resnet50\n",
      "densenet121\n",
      "densenet161\n",
      "densenet169\n",
      "densenet201\n",
      "efficientnet_b0\n",
      "efficientnet_b1\n",
      "efficientnet_b2\n",
      "efficientnet_b3\n",
      "efficientnet_b4\n",
      "efficientnet_b5\n",
      "efficientnet_b6\n",
      "efficientnet_b7\n",
      "fcn_resnet101\n",
      "fcn_resnet50\n",
      "googlenet\n",
      "inception_v3\n",
      "lraspp_mobilenet_v3_large\n",
      "mnasnet0_5\n",
      "mnasnet0_75\n",
      "mnasnet1_0\n",
      "mnasnet1_3\n",
      "mobilenet_v2\n",
      "mobilenet_v3_large\n",
      "mobilenet_v3_small\n",
      "raft_large\n",
      "raft_small\n",
      "regnet_x_16gf\n",
      "regnet_x_1_6gf\n",
      "regnet_x_32gf\n",
      "regnet_x_3_2gf\n",
      "regnet_x_400mf\n",
      "regnet_x_800mf\n",
      "regnet_x_8gf\n",
      "regnet_y_128gf\n",
      "regnet_y_16gf\n",
      "regnet_y_1_6gf\n",
      "regnet_y_32gf\n",
      "regnet_y_3_2gf\n",
      "regnet_y_400mf\n",
      "regnet_y_800mf\n",
      "regnet_y_8gf\n",
      "resnet101\n",
      "resnet152\n",
      "resnet18\n",
      "resnet34\n",
      "resnet50\n",
      "resnext101_32x8d\n",
      "resnext50_32x4d\n",
      "shufflenet_v2_x0_5\n",
      "shufflenet_v2_x1_0\n",
      "squeezenet1_0\n",
      "squeezenet1_1\n",
      "vgg11\n",
      "vgg11_bn\n",
      "vgg13\n",
      "vgg13_bn\n",
      "vgg16\n",
      "vgg16_bn\n",
      "vgg19\n",
      "vgg19_bn\n",
      "vit_b_16\n",
      "vit_b_32\n",
      "vit_l_16\n",
      "vit_l_32\n",
      "wide_resnet101_2\n",
      "wide_resnet50_2\n"
     ]
    }
   ],
   "source": [
    "entrypoints = torch.hub.list('pytorch/vision:v0.12.0', force_reload=True)\n",
    "\n",
    "for e in entrypoints:\n",
    "    # if \"resnet\" in e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/sourove/.cache/torch/hub/pytorch_vision_v0.12.0\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('pytorch/vision:v0.12.0',\n",
    "                         'vit_b_16',\n",
    "                         pretrained = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VisionTransformer(\n",
      "  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
      "  (encoder): Encoder(\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): Sequential(\n",
      "      (encoder_layer_0): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_1): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_2): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_3): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_4): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_5): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_6): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_7): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_8): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_9): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_10): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (encoder_layer_11): EncoderBlock(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (self_attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "        )\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "        (mlp): MLPBlock(\n",
      "          (linear_1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (act): GELU(approximate='none')\n",
      "          (dropout_1): Dropout(p=0.0, inplace=False)\n",
      "          (linear_2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (dropout_2): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (heads): Sequential(\n",
      "    (head): Linear(in_features=768, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = torch.nn.Conv2d(1,1, kernel_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[[[-0.0369,  0.1649, -0.3195],\n",
       "          [-0.1221, -0.1108, -0.1751],\n",
       "          [-0.1359,  0.1959, -0.1193]]]], requires_grad=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchpl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
